# A global singularity image to be used for all jobs - need to specify --use-singularity and have singularity available on the command line
# This image already contains the bioinformatic tools we will be using singularity:
#"file:///shared/.singularity/nextflow-embl-abr-webinar.simg", 
"docker://rsuchecki/nextflow-embl-abr-webinar"

configfile: "config.yaml"

SAMPLES1           = config["SAMPLES1"]
SAMPLES2           = config["SAMPLES2a"]
SAMPLES3           = config["SAMPLES3"]
SAMPLES4           = config["SAMPLES4"]
SAMPLES5           = config["SAMPLES5"]
READS1             = config["READS1"]
READS3             = config["READS2"]
ENVS               = config["ENVS"]
REFERENCES	   = config["REFERENCES"]
LOGS               = config["LOGS"]
DIR                = config["DIR9"]
GENE_MODEL_RNA     = config["GENE_MODEL7"]
GENE_MODEL_PROTEIN = config["GENE_MODEL8"]
GENE_MODEL	   = config["GENE_MODEL9"]
ANNOTATION	   = config["ANNOTATION8"]
COUNTS             = config["COUNTS9"]
MULTIQC            = config["MULTIQC9"]
MAPPED             = config["MAPPED9"]
BAM1               = config["BAM9_1"]
BAM2               = config["BAM9_2"]
BAM3               = config["BAM9_3"]
BAM4               = config["BAM9_4"]
BAM5               = config["BAM9_5"]
NON_CODING         = config["NON_CODING"] 
CLEAN1a            = config["FASTQ_CLEAN1a"]
CLEAN1b            = config["FASTQ_CLEAN1b"]
CLEAN2             = config["FASTQ_CLEAN2"]
CLEAN3             = config["FASTQ_CLEAN3"]
CLEAN4             = config["FASTQ_CLEAN4"]
CLEAN5a            = config["FASTQ_CLEAN5a"]
CLEAN5b            = config["FASTQ_CLEAN5b"]




################
# Pseudo-rules #
################
# By convention, the first rule should be called "all" and it's "input" defined as
# the list of ALL the files you want the workflow to create. e.g.:
rule all:
	input:
		GENE_MODEL+"/gene_model_RNA_exon.gff3",
		GENE_MODEL+"/candidate_ncRNA.gff3",
		GENE_MODEL+"/candidate_ncRNA_inter_anti.gff3",
		GENE_MODEL+"/candidate_ncRNA_intergenic.gff3",
		GENE_MODEL+"/protein_noRNAdata.gff3",
		GENE_MODEL+"/candidate_ncRNA_intronicexonic.gff3",
		GENE_MODEL+"/candidate_ncRNA_antisense.gff3",
		GENE_MODEL+"/transcript/sncRNA.gff3",
		GENE_MODEL+"/transcript/sncRNA_intergenic.gff3",
		GENE_MODEL+"/transcript/sncRNA_intronicexonic.gff3",
		GENE_MODEL+"/transcript/sncRNA_antisense.gff3",
		GENE_MODEL+"/transcript/lncRNA.gff3",
		GENE_MODEL+"/transcript/lncRNA_intergenic.gff3",
		GENE_MODEL+"/transcript/lncRNA_intronicexonic.gff3",
		GENE_MODEL+"/transcript/lncRNA_antisense.gff3",
		expand(GENE_MODEL+"/transcript/{SAMPLE}.fasta", SAMPLE=NON_CODING),
		GENE_MODEL+"/rnacentral/rnacentral_species_specific_ids.fasta.nal",
		expand(GENE_MODEL+"/transcript/{SAMPLE}_blastn.outfmt6", SAMPLE=NON_CODING),
		#GENE_MODEL+"/gene_model_merge.gff3",
		#directory(DIR+"/index"),
		#expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES1),
		#expand(BAM2+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES2),
		#expand(BAM3+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES3),
		#expand(BAM4+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES4),
		#expand(BAM5+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES5),
		#COUNTS+"/counts.out",
		#MULTIQC+"/multiqc_report.html",
		#expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES1),
		#expand(BAM2+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES2),
		#expand(BAM3+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES3),
		#expand(BAM4+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES4),
		#expand(BAM5+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES5),

################
# Rules Proper #
################

rule gene_model_RNA:
	input:
		GENE_MODEL_PROTEIN+"/gene_model_RNA.gff3",
	output:
		GENE_MODEL+"/gene_model_RNA_exon.gff3",
	shell:
		"""
		sed -e "s/match/exon/g" {input} > {output}
		"""

rule compare_gene_model:
	input:
		rna = GENE_MODEL+"/gene_model_RNA_exon.gff3",
		protein = GENE_MODEL_PROTEIN+"/gene_model_protein.gff3",
	output:
		candidate_nc_all = GENE_MODEL+"/candidate_ncRNA.gff3",
		candidate_nc_inter_anti = GENE_MODEL+"/candidate_ncRNA_inter_anti.gff3",
		candidate_nc_intergenic = GENE_MODEL+"/candidate_ncRNA_intergenic.gff3",
		absence_features = GENE_MODEL+"/absence_features.gff3"
	conda:
		ENVS
	shell:
		"""
		bedtools intersect -a {input.rna} -b {input.protein} -v -s -f 1 -F 1 > {output.candidate_nc_all} &&
		bedtools intersect -a {input.rna} -b {input.protein} -v -s > {output.candidate_nc_inter_anti} &&
		bedtools intersect -a {input.rna} -b {input.protein} -v > {output.candidate_nc_intergenic} &&
		bedtools intersect -a {input.protein} -b {input.rna} -v -s -f 1 -F 1 > {output.absence_features}
		"""

rule group_ncRNA:
	input:
		candidate_nc_all = GENE_MODEL+"/candidate_ncRNA.gff3",
		candidate_nc_inter_anti = GENE_MODEL+"/candidate_ncRNA_inter_anti.gff3",
		candidate_nc_intergenic = GENE_MODEL+"/candidate_ncRNA_intergenic.gff3",
	output:
		candidate_nc_introexo = GENE_MODEL+"/candidate_ncRNA_intronicexonic.gff3",
		candidate_nc_antisense = GENE_MODEL+"/candidate_ncRNA_antisense.gff3",
	conda:
		ENVS
	shell:
		"""
		bedtools intersect -a {input.candidate_nc_all} -b {input.candidate_nc_inter_anti} -v -s -f 1 -F 1 > {output.candidate_nc_introexo} && 
		bedtools intersect -a {input.candidate_nc_inter_anti} -b {input.candidate_nc_intergenic} -v -s -f 1 -F 1 > {output.candidate_nc_antisense}
		"""

rule group_lncRNA:
	input:
		ncRNA = GENE_MODEL+"/candidate_ncRNA.gff3",
		ncRNA1 = GENE_MODEL+"/candidate_ncRNA_intergenic.gff3",
		ncRNA2 = GENE_MODEL+"/candidate_ncRNA_intronicexonic.gff3",
		ncRNA3 = GENE_MODEL+"/candidate_ncRNA_antisense.gff3"
	output:
		lncRNA = GENE_MODEL+"/transcript/lncRNA.gff3",
		lncRNA1 = GENE_MODEL+"/transcript/lncRNA_intergenic.gff3",
		lncRNA2 = GENE_MODEL+"/transcript/lncRNA_intronicexonic.gff3",
		lncRNA3 = GENE_MODEL+"/transcript/lncRNA_antisense.gff3"
	shell:
		"""
		awk "\$5-\$4>=200" {input.ncRNA} > {output.lncRNA} &&
		awk "\$5-\$4>=200" {input.ncRNA1} > {output.lncRNA1} &&
		awk "\$5-\$4>=200" {input.ncRNA2} > {output.lncRNA2} &&
		awk "\$5-\$4>=200" {input.ncRNA3} > {output.lncRNA3}
		"""

rule group_sncRNA:
	input:
		ncRNA = GENE_MODEL+"/candidate_ncRNA.gff3",
		ncRNA1 = GENE_MODEL+"/candidate_ncRNA_intergenic.gff3",
		ncRNA2 = GENE_MODEL+"/candidate_ncRNA_intronicexonic.gff3",
		ncRNA3 = GENE_MODEL+"/candidate_ncRNA_antisense.gff3"
	output:
		sncRNA = GENE_MODEL+"/transcript/sncRNA.gff3",
		sncRNA1 = GENE_MODEL+"/transcript/sncRNA_intergenic.gff3",
		sncRNA2 = GENE_MODEL+"/transcript/sncRNA_intronicexonic.gff3",
		sncRNA3 = GENE_MODEL+"/transcript/sncRNA_antisense.gff3"
	shell:
		"""
		awk "\$5-\$4<100" {input.ncRNA} > {output.sncRNA} &&
		awk "\$5-\$4<100" {input.ncRNA1} > {output.sncRNA1} &&
		awk "\$5-\$4<100" {input.ncRNA2} > {output.sncRNA2} &&
		awk "\$5-\$4<100" {input.ncRNA3} > {output.sncRNA3}
		"""

rule protein_noRNAdata:
	input:
		GENE_MODEL+"/absence_features.gff3"
	output:
		GENE_MODEL+"/protein_noRNAdata.gff3"
	shell:
		"""
		grep "exon" {input} > {output}
		"""

rule ncRNA_fasta:
	input:
		references = REFERENCES+"/Plantago.fasta.gz",
		ncRNA = GENE_MODEL+"/transcript/{SAMPLE}.gff3",
	output:
		ncRNA = GENE_MODEL+"/transcript/{SAMPLE}.fasta",
	conda:
		ENVS
	log:
		LOGS+"/{SAMPLE}.log",
	shell:
		"""
		bedtools getfasta -fullHeader -fo {output.ncRNA} -fi {input.references} -bed {input.ncRNA} 
		2> {log}
		"""

rule database_ncRNA:
	input:
		GENE_MODEL+"/rnacentral/rnacentral_species_specific_ids.fasta"
	output:
		GENE_MODEL+"/rnacentral/rnacentral_species_specific_ids.fasta.nal"
	conda:
		ENVS
	shell:
		"""
		makeblastdb -in {input} -dbtype nucl
		"""

rule blastn:
	input:
		GENE_MODEL+"/transcript/{SAMPLE}.fasta"
	output:
		GENE_MODEL+"/transcript/{SAMPLE}_blastn.outfmt6"
	conda:
		ENVS
	params:
		GENE_MODEL+"/rnacentral/rnacentral_species_specific_ids.fasta"
	shell:
		"""
		blastn -query {input} -db {params} -max_target_seqs 1 \
		-outfmt '6 qseqid sseqid stitle pident length mismatch gapopen qstart qend sstart send evalue bitscore' \
		-evalue 1e-3 -num_threads 8 -out {output}
		"""

#rule merge_gene_model:
#	input:
#		file1 = GENE_MODEL+"/gene_model_RNA_exon.gff3",
#		file2 = GENE_MODEL+"/protein_noRNAdata.gff3"
#	output:
#		GENE_MODEL+"/gene_model_merge.gff3"
#	shell:
#		"""
#		cat {input.file1} {input.file2} > {output}
#		"""

rule star_index:
	input:
		fasta = REFERENCES+"/Plantago.fasta.gz",
		gff = GENE_MODEL+"/gene_model_merge.gff3"
	output:
		directory(DIR+"/index")
	conda:
		ENVS
	log:
		LOGS+"/star_index"
	shell:
		"""
		mkdir {output} &&
		STAR --runThreadN 8 \
		--runMode genomeGenerate \
		--genomeDir {output} \
		--genomeSAindexNbases 12 \
		--genomeFastaFiles {input.fasta} \
		--sjdbGTFfile {input.gff} \
		--sjdbGTFtagExonParentTranscript Parent \
		--sjdbGTFfeatureExon exon \
		--sjdbOverhang 520 \
		> {log}
		"""

rule star_mapping1:
	input:
		file1 = CLEAN1a ,
		file2 = CLEAN1b ,
		index = directory(DIR+"/index"),
		gff = GENE_MODEL+"/gene_model_merge.gff3"
	output:
		out1 = BAM1+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM1+"/{SAMPLES}Log.final.out",
		out3 = BAM1+"/{SAMPLES}Log.out",
		out4 = BAM1+"/{SAMPLES}Log.progress.out",
		out5 = BAM1+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM1 ,
		outfile = BAM1+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file1} {input.file2} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gff} \
		--sjdbGTFtagExonParentTranscript Parent \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical
		"""

rule star_mapping2:
	input:
		file = CLEAN2 ,
		index = directory(DIR+"/index"),
		gff = GENE_MODEL+"/gene_model_merge.gff3"
	output:
		out1 = BAM2+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM2+"/{SAMPLES}Log.final.out",
		out3 = BAM2+"/{SAMPLES}Log.out",
		out4 = BAM2+"/{SAMPLES}Log.progress.out",
		out5 = BAM2+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM2 ,
		outfile = BAM2+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gff} \
		--sjdbGTFtagExonParentTranscript Parent \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical
		"""

rule star_mapping3:
	input:
		file = CLEAN3 ,
		index = directory(DIR+"/index"),
		gff = GENE_MODEL+"/gene_model_merge.gff3"
	output:
		out1 = BAM3+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM3+"/{SAMPLES}Log.final.out",
		out3 = BAM3+"/{SAMPLES}Log.out",
		out4 = BAM3+"/{SAMPLES}Log.progress.out",
		out5 = BAM3+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM3 ,
		outfile = BAM3+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gff} \
		--sjdbGTFtagExonParentTranscript Parent \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical \
		"""

rule star_mapping4:
	input:
		file = CLEAN4 ,
		index = directory(DIR+"/index"),
		gff = GENE_MODEL+"/gene_model_merge.gff3",
	output:
		out1 = BAM4+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM4+"/{SAMPLES}Log.final.out",
		out3 = BAM4+"/{SAMPLES}Log.out",
		out4 = BAM4+"/{SAMPLES}Log.progress.out",
		out5 = BAM4+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM4 ,
		outfile = BAM4+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gff} \
		--sjdbGTFtagExonParentTranscript Parent \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical \
		"""

rule star_mapping5:
	input:
		file1 = CLEAN5a ,
		file2 = CLEAN5b ,
		index = directory(DIR+"/index"),
		gff = GENE_MODEL+"/gene_model_merge.gff3"
	output:
		out1 = BAM5+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM5+"/{SAMPLES}Log.final.out",
		out3 = BAM5+"/{SAMPLES}Log.out",
		out4 = BAM5+"/{SAMPLES}Log.progress.out",
		out5 = BAM5+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM5 ,
		outfile = BAM5+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file1} {input.file2} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gff} \
		--sjdbGTFtagExonParentTranscript Parent \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical \
		"""

rule counts:
	input:
		bam = expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES1),
		gff = GENE_MODEL+"/gene_model_merge.gff3",
	output:
		COUNTS+"/counts.out",
	conda:
		ENVS
	log:
		LOGS+"/counts.summary"
	params:
		COUNTS
	shell:
		"""
		featureCounts -Q 10 -g Parent --fracOverlap 1 -s 2 -p -T 5 -F GFF -a {input.gff} -o {output} {input.bam} \
		> {log}
		"""

rule multiqc_all:
	input:
		log1 = expand(BAM1+"/{SAMPLE}Log.out", SAMPLE=SAMPLES1),
		log2 = expand(BAM2+"/{SAMPLE}Log.out", SAMPLE=SAMPLES2),
		log3 = expand(BAM3+"/{SAMPLE}Log.out", SAMPLE=SAMPLES3),
		log4 = expand(BAM4+"/{SAMPLE}Log.out", SAMPLE=SAMPLES4),
		log5 = expand(BAM5+"/{SAMPLE}Log.out", SAMPLE=SAMPLES5),
		counts = rules.counts.output
	output:
		MULTIQC+"/multiqc_report.html"
	conda:
		ENVS
	log:
		LOGS+"/multiqc_report"
	params:
		indir1 = BAM1 ,
		indir2 = BAM2 ,
		indir3 = BAM3 ,
		indir4 = BAM4 ,
		indir5 = BAM5 ,
		indir6 = COUNTS ,
		outdir = MULTIQC
	shell:
		"""
		multiqc {params.indir1} {params.indir2} {params.indir3} {params.indir4} {params.indir5} {params.indir6} -o {params.outdir}
		2> {log}
		"""

rule samtools_bai1:
	input:
		BAM1+"/{SAMPLE}.bam"
	output:
		BAM1+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai2:
	input:
		BAM2+"/{SAMPLE}.bam"
	output:
		BAM2+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai3:
	input:
		BAM3+"/{SAMPLE}.bam"
	output:
		BAM3+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai4:
	input:
		BAM4+"/{SAMPLE}.bam"
	output:
		BAM4+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai5:
	input:
		BAM5+"/{SAMPLE}.bam"
	output:
		BAM5+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""
