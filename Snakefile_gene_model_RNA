# A global singularity image to be used for all jobs - need to specify --use-singularity and have singularity available on the command line
# This image already contains the bioinformatic tools we will be using singularity:
#"file:///shared/.singularity/nextflow-embl-abr-webinar.simg", 
"docker://rsuchecki/nextflow-embl-abr-webinar"

SAMPLES1 = [
"WT_8_DPA_4_S1",
"WT_8_DPA_5_S2",
"WT_8_DPA_6_S3",
"WT_10_DPA_4_S4",
"WT_10_DPA_6_S6",
"WT_12_DPA_4_S7",
"WT_12_DPA_5_S8",
"WT_12_DPA_6_S9",
"WT_14_DPA_4_S10",
"WT_14_DPA_5_S11",
"WT_14_DPA_6_S12",
"1078_6_8_DPA_2_S13",
"1078_6_8_DPA_3_S14",
"1078_6_10_DPA_1_S16",
"1078_6_10_DPA_3_S17",
"1078_6_10_DPA_4_S18",
"1078_6_12_DPA_6_S19",
"1078_6_12_DPA_4_S20",
"1078_6_12_DPA_5_S21",
"1078_6_14_DPA_4_S22",
"1078_6_14_DPA_5_S23",
"1078_6_14_DPA_1_S24"]

#Note: S5 and S15 are discarded due to ribosomal contamination reaching >90%

SAMPLES2 = [
"7_C7N8UANXX_GAGTGG_L006",
"8_C7N8UANXX_ACTGAT_L006",
"9_C7N8UANXX_ATTCCT_L006",
"10_C7N8UANXX_ATCACG_L006",
"7_C7N8UANXX_GAGTGG_L007",
"8_C7N8UANXX_ACTGAT_L007",
"9_C7N8UANXX_ATTCCT_L007",
"10_C7N8UANXX_ATCACG_L007",
"SRR5434206",
"SRR5434207",
#"SRR5434208",
#"SRR5434209",
"SRR5434210",
"SRR5434211",
#"SRR5434212",
#"SRR5434213"
]

SAMPLES3 = [
"JPo_1_C5697ACXX_CCGTCC_L005",
"JPo_2_C5697ACXX_GTCCGC_L005",
"JPo_3_C5697ACXX_GTGAAA_L005",
"JPo_4_C5697ACXX_GTGGCC_L005",
"SRR3883619",
"SRR3883620",
"SRR3883621",
"SRR3883622",
"SRR3885726",
"SRR3885727",
"SRR3885728",
"SRR3883618"]

#Note: Four samples are discarded due to low mapping (leaf infected)

SAMPLES4 = [
#"SRR066373",
#"SRR1311174",
"SRR066374",
#"SRR1311175",
#"SRR342350",
#"SRR342351",
#"SRR066375",
#"SRR1311176",
#"SRR066376",
#"SRR1311177"
]

SAMPLES5 = "SRR629688"

READS1 = ["R1_001", "R2_001"]
READS3 = "R1"

DIR0 = "/fast/users/a1697274/snakemake/bioinformatics"
DIR = DIR0+"/gene_model_RNA"
REFERENCES = DIR0+"/references"
ENVS = DIR0+"/envs/default.yaml"
LOGS = DIR+"/logs"
GENE_MODEL = DIR+"/gene_model"
COUNTS = DIR+"/counts"
MULTIQC = DIR+"/multiqc_all"
MAPPED = DIR+"/mapped_reads"
BAM1 = MAPPED+"/paired_stranded_illumina"
BAM2 = MAPPED+"/single_stranded_illumina"
BAM3 = MAPPED+"/single_unstranded_illumina"
BAM4 = MAPPED+"/single_unstranded_454"
BAM5 = MAPPED+"/paired_unstranded_illumina"
CLEAN1a = DIR0+"/paired_stranded_illumina/clean_reads/{SAMPLES}_R1_001.fastq.gz"
CLEAN1b = DIR0+"/paired_stranded_illumina/clean_reads/{SAMPLES}_R2_001.fastq.gz"
CLEAN2 = DIR0+"/single_stranded_illumina/clean_reads/{SAMPLES}_R1.fastq.gz"
CLEAN3 = DIR0+"/single_unstranded_illumina/clean_reads/{SAMPLES}_R1.fastq.gz"
CLEAN4 = DIR0+"/single_unstranded_454/supersuperclean_reads/{SAMPLES}_R1.fastq.gz"
CLEAN5a = DIR0+"/paired_unstranded_illumina/clean_reads/{SAMPLES}_R1_001.fastq.gz"
CLEAN5b = DIR0+"/paired_unstranded_illumina/clean_reads/{SAMPLES}_R2_001.fastq.gz"


################
# Pseudo-rules #
################
# By convention, the first rule should be called "all" and it's "input" defined as
# the list of ALL the files you want the workflow to create. e.g.:
rule all:
	input:
		GENE_MODEL+"/gene_model_RNA.gtf",
		directory(DIR+"/index"),
		expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES1),
		expand(BAM2+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES2),
		expand(BAM3+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES3),
		expand(BAM4+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES4),
		expand(BAM5+"/{SAMPLE}Aligned.sortedByCoord.out.bam", SAMPLE=SAMPLES5),
		expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES1),
		expand(BAM2+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES2),
		expand(BAM3+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES3),
		expand(BAM4+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES4),
		expand(BAM5+"/{SAMPLE}Aligned.sortedByCoord.out.bam.bai", SAMPLE=SAMPLES5),
		expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.sorted.bam", SAMPLE=SAMPLES1),
		COUNTS+"/counts.out",
		COUNTS+"/counts_2.out",
		MULTIQC+"/multiqc_report.html",

################
# Rules Proper #
################

rule merge_assemblies:
	input:
		GENE_MODEL+"/assemblies.txt"
	output:
		GENE_MODEL+"/gene_model_RNA.gtf"
	conda:
		ENVS
	params: 
		ref = REFERENCES+"/Plantago.fasta",
		merged = GENE_MODEL ,
		output = GENE_MODEL+"/merged.gtf" 
	shell:
		"""
		cuffmerge -p 5 -o {params.merged} -s {params.ref} {input} &&
		mv {params.output} {output}
		"""

rule star_index:
	input:
		fasta = REFERENCES+"/Plantago.fasta",
		gtf = GENE_MODEL+"/gene_model_RNA.gtf"
	output:
		directory(DIR+"/index")
	conda:
		ENVS
	log:
		LOGS+"/star_index"
	shell:
		"""
		mkdir {output} &&
		STAR --runThreadN 8 \
		--runMode genomeGenerate \
		--genomeDir {output} \
		--genomeSAindexNbases 12 \
		--genomeFastaFiles {input.fasta} \
		--sjdbGTFfile {input.gtf} \
		--sjdbGTFfeatureExon exon \
		--sjdbOverhang 520 \
		> {log}
		"""

rule star_mapping1:
	input:
		file1 = CLEAN1a ,
		file2 = CLEAN1b ,
		index = directory(DIR+"/index"),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf"
	output:
		out1 = BAM1+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM1+"/{SAMPLES}Log.final.out",
		out3 = BAM1+"/{SAMPLES}Log.out",
		out4 = BAM1+"/{SAMPLES}Log.progress.out",
		out5 = BAM1+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM1 ,
		outfile = BAM1+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file1} {input.file2} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gtf} \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical
		"""

rule star_mapping2:
	input:
		file = CLEAN2 ,
		index = directory(DIR+"/index"),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf"
	output:
		out1 = BAM2+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM2+"/{SAMPLES}Log.final.out",
		out3 = BAM2+"/{SAMPLES}Log.out",
		out4 = BAM2+"/{SAMPLES}Log.progress.out",
		out5 = BAM2+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM2 ,
		outfile = BAM2+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gtf} \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical
		"""

rule star_mapping3:
	input:
		file = CLEAN3 ,
		index = directory(DIR+"/index"),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf"
	output:
		out1 = BAM3+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM3+"/{SAMPLES}Log.final.out",
		out3 = BAM3+"/{SAMPLES}Log.out",
		out4 = BAM3+"/{SAMPLES}Log.progress.out",
		out5 = BAM3+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM3 ,
		outfile = BAM3+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gtf} \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical \
		"""

rule star_mapping4:
	input:
		file = CLEAN4 ,
		index = directory(DIR+"/index"),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf"
	output:
		out1 = BAM4+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM4+"/{SAMPLES}Log.final.out",
		out3 = BAM4+"/{SAMPLES}Log.out",
		out4 = BAM4+"/{SAMPLES}Log.progress.out",
		out5 = BAM4+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM4 ,
		outfile = BAM4+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gtf} \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical \
		"""

rule star_mapping5:
	input:
		file1 = CLEAN5a ,
		file2 = CLEAN5b ,
		index = directory(DIR+"/index"),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf"
	output:
		out1 = BAM5+"/{SAMPLES}Aligned.sortedByCoord.out.bam",
		out2 = BAM5+"/{SAMPLES}Log.final.out",
		out3 = BAM5+"/{SAMPLES}Log.out",
		out4 = BAM5+"/{SAMPLES}Log.progress.out",
		out5 = BAM5+"/{SAMPLES}SJ.out.tab"
	conda:
		ENVS
	params:
		bam = BAM5 ,
		outfile = BAM5+"/{SAMPLES}"
	shell:
		"""
		mkdir --parents {params.bam} &&
		STAR --runThreadN 8 \
		--genomeDir {input.index} \
		--readFilesIn {input.file1} {input.file2} \
		--readFilesCommand gunzip -c \
		--outFileNamePrefix {params.outfile} \
		--outSAMtype BAM SortedByCoordinate \
		--alignIntronMax 10000 \
		--sjdbGTFfile {input.gtf} \
		--sjdbGTFfeatureExon exon \
		--outFilterScoreMinOverLread 0.8 \
		--outSJfilterCountUniqueMin 5 1 1 1 \
		--outSJfilterOverhangMin 35 20 20 20 \
		--outFilterIntronMotifs RemoveNoncanonical \
		"""

rule sort:
	input:
		BAM1+"/{SAMPLE}.bam",
	output:
		BAM1+"/{SAMPLE}.sorted.bam"
	conda:
		ENVS
	params:
		BAM1+"/{SAMPLE}.sorted"
	shell:
		"""
		samtools sort -l 9 -n -T {params} -o {output} {input}
		"""

rule counts:
	input:
		bam = expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.sorted.bam", SAMPLE=SAMPLES1),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf",
	output:
		COUNTS+"/counts.out",
	conda:
		ENVS
	log:
		LOGS+"/counts.summary"
	params:
		COUNTS
	shell:
		"""
		featureCounts -t exon -g gene_id -f -O -s 2 -p -T 5 -a {input.gtf} -o {output} {input.bam} \
		> {log}
		"""

rule counts_2:
	input:
		bam = expand(BAM1+"/{SAMPLE}Aligned.sortedByCoord.out.sorted.bam", SAMPLE=SAMPLES1),
		gtf = GENE_MODEL+"/gene_model_RNA.gtf",
	output:
		COUNTS+"/counts_2.out",
	conda:
		ENVS
	log:
		LOGS+"/counts_2.summary"
	params:
		COUNTS
	shell:
		"""
		featureCounts -Q 10 -t exon --fracOverlap 1 -s 2 -p -T 5 -a {input.gtf} -o {output} {input.bam} \
		> {log}
		"""

rule multiqc_all:
	input:
		log1 = expand(BAM1+"/{SAMPLE}Log.final.out", SAMPLE=SAMPLES1),
		log2 = expand(BAM2+"/{SAMPLE}Log.final.out", SAMPLE=SAMPLES2),
		log3 = expand(BAM3+"/{SAMPLE}Log.final.out", SAMPLE=SAMPLES3),
		log4 = expand(BAM4+"/{SAMPLE}Log.final.out", SAMPLE=SAMPLES4),
		log5 = expand(BAM5+"/{SAMPLE}Log.final.out", SAMPLE=SAMPLES5),
	output:
		MULTIQC+"/multiqc_report.html"
	conda:
		ENVS
	log:
		LOGS+"/multiqc_report"
	params:
		indir1 = BAM1 ,
		indir2 = BAM2 ,
		indir3 = BAM3 ,
		indir4 = BAM4 ,
		indir5 = BAM5 ,
		outdir = MULTIQC
	shell:
		"""
		multiqc {params.indir1} {params.indir2} {params.indir3} {params.indir4} {params.indir5} -o {params.outdir}
		2> {log}
		"""

rule samtools_bai1:
	input:
		BAM1+"/{SAMPLE}.bam"
	output:
		BAM1+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai2:
	input:
		BAM2+"/{SAMPLE}.bam"
	output:
		BAM2+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai3:
	input:
		BAM3+"/{SAMPLE}.bam"
	output:
		BAM3+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai4:
	input:
		BAM4+"/{SAMPLE}.bam"
	output:
		BAM4+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""

rule samtools_bai5:
	input:
		BAM5+"/{SAMPLE}.bam"
	output:
		BAM5+"/{SAMPLE}.bam.bai"
	conda:
		ENVS
	params:
		""
	shell:
		"""
		samtools index {params} {input} {output}
		"""
